{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XanimGuliyeva/Document_Similarity_Checker/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasketch\n",
        "import datasketch"
      ],
      "metadata": {
        "id": "dfWGQPH4BgpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gdown\n",
        "import numpy as np\n",
        "import tarfile\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from datasketch import MinHash, MinHashLSH\n",
        "\n",
        "\n",
        "# Google Drive file ID and output file\n",
        "file_id = '1AIwNIs40Ix3kpEXzzVJqmdaT5Twktfl7'\n",
        "output = '20_newsgroups.tar.gz'\n",
        "\n",
        "# Download the tar.gz file\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', output, quiet=False)\n",
        "\n",
        "# Extract the tar.gz file\n",
        "with tarfile.open(output, 'r:gz') as tar:\n",
        "    tar.extractall('20_newsgroups')\n",
        "\n",
        "# # Function to load documents from local directories\n",
        "# def load_documents_from_directory(directory_path):\n",
        "#     documents = []\n",
        "#     for root, dirs, files in os.walk(directory_path):\n",
        "#         for file in files:\n",
        "#             if file.endswith('.txt'):\n",
        "#                 file_path = os.path.join(root, file)\n",
        "#                 with open(file_path, 'r', encoding='utf-8') as f:\n",
        "#                     documents.append(f.read())\n",
        "#     return documents\n"
      ],
      "metadata": {
        "id": "OUCh7ab96zXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the extracted dataset directory\n",
        "dataset_path = '20_newsgroups/20_newsgroups'\n",
        "\n",
        "# Function to list all files in the dataset directory\n",
        "def list_files_in_directory(directory_path):\n",
        "    file_list = []\n",
        "    for root, dirs, files in os.walk(directory_path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            file_list.append(file_path)\n",
        "    return file_list\n",
        "\n",
        "# List all files in the dataset directory\n",
        "file_list = list_files_in_directory(dataset_path)\n",
        "print(f\"Total number of files: {len(file_list)}\")"
      ],
      "metadata": {
        "id": "JHR46TixB_Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "#token.isalnum(): Checks if the token consists only of alphanumeric characters (letters and digits).\n",
        "# Function to tokenize text and ignore stop words\n",
        "def tokenize(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words and token.isalnum()]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Function to generate shingles\n",
        "def get_shingles(tokens, k=9):\n",
        "    shingles = set()\n",
        "    for i in range(len(tokens) - k + 1):\n",
        "        shingle = ' '.join(tokens[i:i+k])\n",
        "        shingles.add(shingle)\n",
        "    return shingles\n",
        "\n",
        "# Function to compute Minhash signature\n",
        "def compute_minhash(shingles, num_hashes=100):\n",
        "    m = MinHash(num_perm=num_hashes)\n",
        "    for shingle in shingles:\n",
        "        m.update(shingle.encode('utf8'))\n",
        "    return m\n"
      ],
      "metadata": {
        "id": "1lDZvQARG6la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize list to hold Minhash signatures\n",
        "minhash_signatures = []\n",
        "shingled_documents = []\n",
        "\n",
        "# Set the size of each shingle and number of hash functions\n",
        "shingle_size = 9\n",
        "num_hashes = 100\n",
        "\n",
        "# Function to extract the document ID from the file path\n",
        "def extract_doc_id(file_path):\n",
        "    return os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "# Process each file, tokenize it, create shingles, and compute Minhash\n",
        "for index, file_path in enumerate(file_list):\n",
        "    with open(file_path, 'r', encoding='latin-1') as file:\n",
        "        content = file.read()\n",
        "        tokens = tokenize(content)\n",
        "        shingles = get_shingles(tokens, shingle_size)\n",
        "        shingled_documents.append(shingles)\n",
        "        minhash = compute_minhash(shingles, num_hashes)\n",
        "        minhash_signatures.append(minhash)\n",
        "        doc_id = extract_doc_id(file_path)\n",
        "        if index < 10:\n",
        "            # Print example shingles and Minhash signatures for the first 10 documents\n",
        "            print(f\"Document {doc_id} tokens: {tokens}\")\n",
        "            print(f\"Document {doc_id} shingles: {shingles}\")\n",
        "            print(f\"Document {doc_id} Minhash signature: {minhash.hashvalues}\")\n"
      ],
      "metadata": {
        "id": "N3LhZ4UTm3MX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create LSH index and find candidate pairs with file paths\n",
        "def create_lsh(minhash_signatures, file_list, threshold=0.7, num_hashes=100):\n",
        "    lsh = MinHashLSH(threshold=threshold, num_perm=num_hashes)\n",
        "\n",
        "    # Insert Minhash signatures into LSH\n",
        "    for i, m in enumerate(minhash_signatures):\n",
        "        lsh.insert(f\"doc_{i}\", m)\n",
        "\n",
        "    # Find candidate pairs\n",
        "    candidate_pairs = []\n",
        "    for i, m in enumerate(minhash_signatures):\n",
        "        result = lsh.query(m)\n",
        "        filtered_result = [r for r in result if r != f\"doc_{i}\"]\n",
        "        candidate_pairs.append((f\"doc_{i}\", filtered_result))\n",
        "\n",
        "    # Map document IDs to file paths\n",
        "    doc_id_to_path = {f\"doc_{i}\": file_list[i] for i in range(len(file_list))}\n",
        "\n",
        "    return candidate_pairs, doc_id_to_path\n",
        "\n",
        "\n",
        "\n",
        "# Assuming `minhash_signatures` and `file_list` are already defined and populated\n",
        "candidate_pairs, doc_id_to_path = create_lsh(minhash_signatures, file_list, threshold=0.7, num_hashes=100)\n",
        "\n",
        "# Print the candidate pairs with their names and paths\n",
        "for doc, pairs in candidate_pairs:\n",
        "    if pairs:  # Check if there are any candidate pairs\n",
        "        doc_path = doc_id_to_path[doc]\n",
        "        doc_name = os.path.basename(doc_path)\n",
        "        paired_docs = [(doc_id_to_path[pair], os.path.basename(doc_id_to_path[pair])) for pair in pairs]\n",
        "        # print(f\"Document {doc_name} (path: {doc_path}) has candidate pairs: {paired_docs}\")\n",
        "        print(f\"Document {doc_name} has candidate pairs: {paired_docs}\")"
      ],
      "metadata": {
        "id": "39xgHzAgnXKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute Jaccard similarity\n",
        "def jaccard_similarity(set1, set2):\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union\n",
        "\n",
        "# Function to compute similar pairs based on Jaccard similarity\n",
        "def compute_similar_pairs(candidate_pairs, shingled_documents, doc_id_to_path, similarity_threshold=0.5):\n",
        "    similar_pairs = []\n",
        "    for doc_id, candidates in candidate_pairs:\n",
        "        doc_index = int(doc_id.split('_')[1])\n",
        "        for candidate_id in candidates:\n",
        "            candidate_index = int(candidate_id.split('_')[1])\n",
        "            #not to compare with itself\n",
        "            if doc_index != candidate_index:\n",
        "                similarity = jaccard_similarity(shingled_documents[doc_index], shingled_documents[candidate_index])\n",
        "                if similarity > similarity_threshold:\n",
        "                    doc_name = os.path.basename(doc_id_to_path[doc_id])\n",
        "                    candidate_name = os.path.basename(doc_id_to_path[candidate_id])\n",
        "                    similar_pairs.append((doc_name, candidate_name, similarity))\n",
        "    return similar_pairs\n",
        "\n",
        "# Compute similar pairs based on Jaccard similarity\n",
        "similar_pairs = compute_similar_pairs(candidate_pairs, shingled_documents, doc_id_to_path, similarity_threshold=0.5)\n",
        "\n",
        "# Print the similar pairs with their names\n",
        "for doc_name, candidate_name, similarity in similar_pairs:\n",
        "    print(f\"Documents {doc_name} and {candidate_name} are similar with Jaccard similarity {similarity:.2f}\")"
      ],
      "metadata": {
        "id": "7ZNV-I8zoKKw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}